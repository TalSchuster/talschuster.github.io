[{"authors":["admin"],"categories":null,"content":" אני חוקר למידת מכונה בחטיבת המחקר של גוגל. סיימתי את הדוקטורט במעבדת הבינה המלאכותית של המכון הטכנולוגי של מסצ׳וסטס תחת הנחייתה של פרופ׳ רגינה ברזילי.  המחקר שלי סוקר מספר פעמים בחדשות בישראל: גיקטיים1, גיקטיים2, גיקטיים3, גלובס, חדשות 12, ופעמים רבות בשאר העולם.  פרטים נוספים באתר באנגלית. ","date":1640995200,"expirydate":-62135596800,"kind":"taxonomy","lang":"he","lastmod":1640995200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://talschuster.github.io/he/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/he/authors/admin/","section":"authors","summary":"אני חוקר למידת מכונה בחטיבת המחקר של גוגל. סיימתי את הדוקטורט במעבדת הבינה המלאכותית של המכון הטכנולוגי של מסצ׳וסטס תחת הנחייתה של פרופ׳ רגינה ברזילי.  המחקר שלי סוקר מספר פעמים בחדשות בישראל: גיקטיים1, גיקטיים2, גיקטיים3, גלובס, חדשות 12, ופעמים רבות בשאר העולם.","tags":null,"title":"Tal Schuster","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://talschuster.github.io/he/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/he/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Vamsi Aribandi","Yi Tay","Tal Schuster","Jinfeng Rao","Huaixiu Steven Zheng","Sanket Vaibhav Mehta","Honglei Zhuang","Vinh Q Tran","Dara Bahri","Jianmo Ni","Jai Gupta","Kai Hui","Sebastian Ruder","Donald Metzler"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1640995200,"objectID":"242c2ba54b0ccb5774e864d329d08ede","permalink":"https://talschuster.github.io/he/publication/ext5/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/he/publication/ext5/","section":"publication","summary":"Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.","tags":[""],"title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning","type":"publication"},{"authors":["Tal Schuster","Ashwin Kalyan","Oleksandr Polozov","Adam Tauman Kalai"],"categories":null,"content":"","date":1623283200,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1623283200,"objectID":"3af23eae025b4325e48a394bfe11f094","permalink":"https://talschuster.github.io/he/publication/puzzles/","publishdate":"2021-06-10T00:00:00Z","relpermalink":"/he/publication/puzzles/","section":"publication","summary":"We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program $f$, and the goal is to find an input $x$ which makes $f$ output \"True\". The puzzles are objective in that each one is specified entirely by the source code of its verifier $f$, so evaluating $f(x)$ is all that is needed to test a candidate solution $x$. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems that are immediately obvious to human programmers (but not necessarily to AI), to classic programming puzzles (e.g., Towers of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). The objective nature of P3 readily supports self-supervised bootstrapping. We develop baseline enumerative program synthesis and GPT-3 solvers that are capable of solving easy puzzles -- even without access to any reference solutions -- by learning from their own past solutions. Based on a small user study, we find puzzle difficulty to correlate between human programmers and the baseline AI solvers.","tags":[""],"title":"Programming Puzzles","type":"publication"},{"authors":["Tal Schuster","Adam Fisch","Tommi Jaakkola","Regina Barzilay"],"categories":null,"content":"","date":1618704000,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1618704000,"objectID":"5378d83722c811009315e3e582d236b8","permalink":"https://talschuster.github.io/he/publication/cats/","publishdate":"2021-04-18T00:00:00Z","relpermalink":"/he/publication/cats/","section":"publication","summary":"We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs -- Confident Adaptive Transformers -- in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original model with high confidence. Our method trains additional prediction heads on top of intermediate layers, and dynamically decides when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks.","tags":[""],"title":"Consistent Accelerated Inference via Confident Adaptive Transformers","type":"publication"},{"authors":["Tal Schuster","Adam Fisch","Regina Barzilay"],"categories":null,"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1615766400,"objectID":"41280200f3c721d6d1b58ac4f8ae68e3","permalink":"https://talschuster.github.io/he/publication/vitaminc/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/he/publication/vitaminc/","section":"publication","summary":"Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness -- improving accuracy by 10% on adversarial fact verification and 6% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.","tags":[""],"title":"Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence","type":"publication"},{"authors":["Adam Fisch","Tal Schuster","Tommi Jaakkola","Regina Barzilay"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1612137600,"objectID":"a9804de45265a8256755466ea1f59388","permalink":"https://talschuster.github.io/he/publication/meta_conformal/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/he/publication/meta_conformal/","section":"publication","summary":"We develop a novel approach to conformal prediction when the target task has limited data available for training. Conformal prediction identifies a small set of promising output candidates in place of a single prediction, with guarantees that the set contains the correct answer with high probability. When training data is limited, however, the predicted set can easily become unusably large. In this work, we obtain substantially tighter prediction sets while maintaining desirable marginal guarantees by casting conformal prediction as a meta-learning paradigm over exchangeable collections of auxiliary tasks. Our conformalization algorithm is simple, fast, and agnostic to the choice of underlying model, learning algorithm, or dataset. We demonstrate the effectiveness of this approach across a number of few-shot classification and regression tasks in natural language processing, computer vision, and computational chemistry for drug discovery.","tags":[""],"title":"Few-shot Conformal Prediction with Auxiliary Tasks","type":"publication"},{"authors":["Adam Fisch","Tal Schuster","Tommi Jaakkola","Regina Barzilay"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1609459200,"objectID":"222dcc3711fca587757e0fc167e1f7d2","permalink":"https://talschuster.github.io/he/publication/relaxed_cascaded_conformal_prediction/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/he/publication/relaxed_cascaded_conformal_prediction/","section":"publication","summary":"In this paper, we present a novel approach for conformal prediction (CP), in which we aim to identify a set of promising prediction candidates---in place of a single prediction. This set is guaranteed to contain a correct answer with high probability, and is well-suited for many open-ended classification tasks. In the standard CP paradigm, the predicted set can often be unusably large and also costly to obtain. This is particularly pervasive in settings where the correct answer is not unique, and the number of total possible answers is high. We first expand the CP correctness criterion to allow for additional, inferred \"admissible\" answers, which can substantially reduce the size of the predicted set while still providing valid performance guarantees. Second, we amortize costs by conformalizing prediction cascades, in which we aggressively prune implausible labels early on by using progressively stronger classifiers---again, while still providing valid performance guarantees. We demonstrate the empirical effectiveness of our approach for multiple applications in natural language processing and computational chemistry for drug discovery.","tags":[""],"title":"Efficient Conformal Prediction via Cascaded Inference with Expanded Admission","type":"publication"},{"authors":["Enrico Santus","Tal Schuster","Amir M Tahmasebi","Clara Li","Adam Yala","Conor R Lanahan","Peter Prinsen","Scott F Thompson","Samuel Coons","Lance Mynderse","Regina Barzilay","Kevin Hughes"],"categories":null,"content":"","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1601596800,"objectID":"72269f8be14ec95f8cda917276a1243c","permalink":"https://talschuster.github.io/he/publication/medical_rules/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/he/publication/medical_rules/","section":"publication","summary":"We collected 501 prostate pathology reports from 6 American hospitals. Reports were split into 2,711 core segments, annotated with 20 attributes describing the histology, grade, extension, and location of tumors. The data set was split by institutions to generate a cross-institutional evaluation setting. We assessed 4 systems, namely a rule-based approach, an ML model, and 2 hybrid systems integrating the previous methods: a Rule as Feature model and a Classifier Confidence model. Several ML algorithms were tested, including logistic regression (LR), support vector machine (SVM), and eXtreme gradient boosting (XGB).","tags":[""],"title":"Exploiting Rules to Enhance Machine Learning in Extracting Information From Multi-Institutional Prostate Pathology Reports","type":"publication"},{"authors":["Beatrice Portelli","Jason Zhao","Tal Schuster","Giuseppe Serra","Enrico Santus"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1593561600,"objectID":"20b19a2a64702f6666ef6e7b7c2ebd27","permalink":"https://talschuster.github.io/he/publication/distil_evidence/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/he/publication/distil_evidence/","section":"publication","summary":"The alarming spread of fake news in social media, together with the impossibility of scaling manual fact verification, motivated the development of natural language processing techniques to automatically verify the veracity of claims. Most approaches perform a claim-evidence classification without providing any insights about why the claim is trustworthy or not. We propose, instead, a model-agnostic framework that consists of two modules: (1) a span extractor, which identifies the crucial information connecting claim and evidence; and (2) a classifier that combines claim, evidence, and the extracted spans to predict the veracity of the claim. We show that the spans are informative for the classifier, improving performance and robustness. Tested on several state-of-the-art models over the Fever dataset, the enhanced classifiers consistently achieve higher accuracy while also showing reduced sensitivity to artifacts in the claims.","tags":[""],"title":"Distilling the Evidence to Augment Fact Verification Models","type":"publication"},{"authors":["Tal Schuster","Roei Schuster","Darsh J Shah","Regina Barzilay"],"categories":null,"content":"Note: this article was earlier published under the name: Are We Safe Yet? The Limitations of Distributional Features for Fake News Detection\n","date":1581724800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1581724800,"objectID":"cc5f5afb81073fae84dc22603f195124","permalink":"https://talschuster.github.io/he/publication/are_we_safe/","publishdate":"2020-02-15T00:00:00Z","relpermalink":"/he/publication/are_we_safe/","section":"publication","summary":"Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. While humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, employed in auto-completion and editing-assistance settings. Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.","tags":[""],"title":"The Limitations of Stylometry for Detecting Machine-Generated Fake News","type":"publication"},{"authors":["Roei Schuster","Tal Schuster","Yoav Meri","Vitaly Shmatikov"],"categories":null,"content":" ","date":1579046400,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1579046400,"objectID":"9eaafb21ed893968079607b179d0191e","permalink":"https://talschuster.github.io/he/publication/emb_attack/","publishdate":"2020-01-15T00:00:00Z","relpermalink":"/he/publication/emb_attack/","section":"publication","summary":"Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word \"meaning\" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks. Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the \"meaning\" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another. An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model.","tags":[""],"title":"Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning","type":"publication"},{"authors":["Darsh J Shah","Tal Schuster","Regina Barzilay"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1577836800,"objectID":"5ea625ac32fa8a1ac46d4980fe1487eb","permalink":"https://talschuster.github.io/he/publication/fact_generation/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/he/publication/fact_generation/","section":"publication","summary":"Online encyclopediae like Wikipedia contain large amounts of text that need frequent corrections and updates. The new information may contradict existing content in encyclopediae. In this paper, we focus on rewriting such dynamically changing articles. This is a challenging constrained generation task, as the output must be consistent with the new information and fit into the rest of the existing document. To this end, we propose a two-step solution: (1) We identify and remove the contradicting components in a target text for a given claim, using a neutralizing stance model; (2) We expand the remaining text to be consistent with the given claim, using a novel two-encoder sequence-to-sequence model with copy attention. Applied to a Wikipedia fact update dataset, our method successfully generates updated sentences for new claims, achieving the highest SARI score. Furthermore, we demonstrate that generating synthetic data through such rewritten sentences can successfully augment the FEVER fact-checking training dataset, leading to a relative error reduction of 13%.","tags":[""],"title":"Automatic Fact-guided Sentence Modification","type":"publication"},{"authors":null,"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1577836800,"objectID":"478fb0dcfdddc34e1e4a4380dc95f197","permalink":"https://talschuster.github.io/he/project/conformal/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/he/project/conformal/","section":"project","summary":"Predicting with confidence estimates","tags":["Vision","NLP","Deep Learning"],"title":"Conformal Prediction","type":"project"},{"authors":["Tal Schuster","Darsh J Shah","Yun Jie Serene Yeo","Daniel Filizzola","Enrico Santus","Regina Barzilay"],"categories":null,"content":"","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1565740800,"objectID":"6870585fdd769be814624a00c87dfb18","permalink":"https://talschuster.github.io/he/publication/fact_symmetric/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/he/publication/fact_symmetric/","section":"publication","summary":"Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","tags":[""],"title":"Towards Debiasing Fact Verification Models","type":"publication"},{"authors":["Adam Yala","Tal Schuster","Randy Miles","Regina Barzilay","Constance Lehman"],"categories":null,"content":"","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1565049600,"objectID":"8a1f8e1a10a54491fee218ea25bc0648","permalink":"https://talschuster.github.io/he/publication/mammo_triage/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/he/publication/mammo_triage/","section":"publication","summary":"A DL model to triage a portion of mammograms as cancer free, improving performance and workflow efficiency.","tags":[""],"title":"A Deep Learning Model to Triage Screening Mammograms: A Simulation Study","type":"publication"},{"authors":["Tal Schuster","Ori Ram","Regina Barzilay","Amir Globerson"],"categories":null,"content":" ","date":1559606400,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1559606400,"objectID":"0a9ee1095508d67446f94b46c8842cdd","permalink":"https://talschuster.github.io/he/publication/crosslingual_embed/","publishdate":"2019-06-04T00:00:00Z","relpermalink":"/he/publication/crosslingual_embed/","section":"publication","summary":"We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the context-dependent spaces. This mapping readily supports processing of a target language, improving transfer by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our method consistently outperforms the previous state-of-the-art on 6 target languages, yielding an improvement of 6.8 LAS points on average.","tags":[""],"title":"Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing","type":"publication"},{"authors":["Adam Yala","Constance Lehman","Tal Schuster","Tally Portnoi","Regina Barzilay"],"categories":null,"content":"","date":1557187200,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1557187200,"objectID":"d3c2c6a0b463ad2965d2743a2e7f43c0","permalink":"https://talschuster.github.io/he/publication/mammo_risk/","publishdate":"2019-05-07T00:00:00Z","relpermalink":"/he/publication/mammo_risk/","section":"publication","summary":"Deep learning models that use full-field mammograms yield substantially improved risk discrimination compared with the Tyrer-Cuzick (version 8) model.","tags":[""],"title":"A Deep Learning Mammography-based Model for Improved Breast Cancer Risk Prediction","type":"publication"},{"authors":["Tally Portnoi","Adam Yala","Tal Schuster","Regina Barzilay","Brian Dontchos","Leslie Lamb","Constance Lehman"],"categories":null,"content":"","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1546819200,"objectID":"80a02bbc76309c194b5fe75a0fb0738c","permalink":"https://talschuster.github.io/he/publication/mri/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/he/publication/mri/","section":"publication","summary":"Our DL model can assess the 5-year cancer risk on the basis of a breast MR image alone, and it showed improved individual risk discrimination when compared with a state-of-the-art risk assessment model. These results offer promising preliminary data regarding the potential of image-based risk assessment models to support more personalized care.","tags":[""],"title":"Deep Learning Model to Assess Cancer Risk on the Basis of a Breast MR Image Alone","type":"publication"},{"authors":["Constance Lehman","Adam Yala","Tal Schuster","Brian Dontchos","Manisha Bahl","Kyle Swanson","Regina Barzilay"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1538352000,"objectID":"be5b47b0cff44c75b70d2255263c14b1","permalink":"https://talschuster.github.io/he/publication/density/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/he/publication/density/","section":"publication","summary":"This DL model can be used to assess mammographic breast density at the level of an experienced mammographer.","tags":[""],"title":"Mammographic breast density assessment using deep learning: clinical implementation","type":"publication"},{"authors":["Tal Schuster","Lior Wolf","David Gadot"],"categories":null,"content":" ","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1498867200,"objectID":"7a4eb34ff58bbdaa22125d4bc053783e","permalink":"https://talschuster.github.io/he/publication/optical/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/he/publication/optical/","section":"publication","summary":"We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.","tags":[""],"title":"Optical Flow Requires Multiple Strategies (but only one network)","type":"publication"},{"authors":null,"categories":null,"content":"In this project, we aim to improve the automatic detection of false information. We rely on trustworthy sources and evaluate content claims them.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1483228800,"objectID":"6183e839850c7dcc636098d8a28bac59","permalink":"https://talschuster.github.io/he/project/fact_verification/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/he/project/fact_verification/","section":"project","summary":"Identifying false or inaccurate facts","tags":["NLP","Deep Learning"],"title":"Fact Verification","type":"project"},{"authors":null,"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1483228800,"objectID":"f8759f2a134e7085f152c4a5b42cd32d","permalink":"https://talschuster.github.io/he/project/optical/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/he/project/optical/","section":"project","summary":"Deep metric learning","tags":["Vision","Deep Learning"],"title":"Learning Strategies","type":"project"},{"authors":null,"categories":null,"content":"Project website \nToday, the vast majority of breast cancers are diagnosed from mammograms. While radiologists are skilled in identifying suspicious areas once cancer is already visible on the image, they have limited capacity in predicting which patients are heading towards cancer diagnosis before mass is formed. Reliably identifying such patients is the crucial first step in preventing cancer development. Substantial research in medical literature confirms that certain changes in the breast tissue are early precursors of cancer development. However, patterns identified by humans are not sufficiently strong to act upon them.\nWe are aiming to automate these predictions by developing deep learning algorithms that can using longitudinal imaging data with known outcomes. By reading millions of images (orders of magnitude more than humans can read in their lifetime), machines should be able to assess the likelihood of cancer occurrence in a more accurate fashion. We are currently collecting a large corpus that will enable us to answer this question. Meanwhile, we are building algorithms that can help radiologists with their daily tasks of mammogram reading. The deep learning models we developed today can accurately predict breast density and identify with human accuracy a large fraction of safe screening mammograms.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1483228800,"objectID":"e2ab62a05f4c5180aabbef46764a1a03","permalink":"https://talschuster.github.io/he/project/mammography/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/he/project/mammography/","section":"project","summary":"Deep learning for cancer risk prediction","tags":["Medical","Deep Learning","Vision"],"title":"Medical","type":"project"},{"authors":null,"categories":null,"content":"Analyzing the space of word representations for multilingual applications and for better performance with less data.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":1483228800,"objectID":"049a11ddb4792fa0a6e07d7ec7e9ee03","permalink":"https://talschuster.github.io/he/project/embeddings/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/he/project/embeddings/","section":"project","summary":"Dense representations of words","tags":["Embeddings","NLP","Deep Learning"],"title":"Word Embeddings","type":"project"},{"authors":null,"categories":null,"content":"               ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"he","lastmod":-62135596800,"objectID":"348109ce63001d1dc146d86ce0cd8625","permalink":"https://talschuster.github.io/he/photography/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/he/photography/","section":"","summary":"               ","tags":null,"title":"Gallery","type":""}]